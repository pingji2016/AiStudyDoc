
# 模型量化

## 参考文档
PyTorch  实现模型量化 https://pytorch-cn.com/tutorials/recipes/quantization.html
PyTorch  实现模型量化 https://docs.pytorch.org/docs/2.8/quantization.html
PyTorch  实现模型量化
https://pytorch.org/blog/introduction-to-quantization-on-pytorch/

好的，这是一个关于模型优化的重要概念。静态量化和动态量化都是用来减少模型大小、提升推理速度的技术，但它们在“何时量化”这一关键步骤上有所不同。

让我们用一个简单的比喻开始：

> **想象一个厨师在做菜（模型推理）**
> *   **动态量化**：厨师在**接到订单后（运行时）**，才看冰箱里有什么食材（激活值），然后决定每道菜用多少盐和酱油（计算标尺）。
> *   **静态量化**：厨师在**开业前（训练后）**，就通过试做大量菜品（校准数据），精确制定好了每道菜的固定配方（固定的标尺），开业后直接按配方做。

下面我们进行详细的解释和对比。

---

### 核心概念：量化是什么？

量化（Quantization）的核心思想是**使用更低精度的数据类型（如int8）来表示和计算原本高精度的数据（如float32）**。
*   **好处**：
    *   **模型变小**：int8占用的空间是float32的1/4，便于部署在移动端和嵌入式设备。
    *   **推理更快**：整数运算通常比浮点运算快得多，尤其是在支持int8指令集的硬件（如ARM NEON, Intel AVX-512）上。
    *   **功耗更低**：减少内存访问和计算强度。
*   **挑战**：
    *   会引入精度损失，因为低精度数据无法完美表示高精度数据的全部信息。

### 动态量化

**1. 核心思想：**
*   **权重（Weights）在模型加载时就提前量化为int8。**
*   **激活（Activations）/ 输入数据 在推理过程中动态地量化为int8。**
    *   “动态”指的是：为**每一次前向传播（每次推理）** 计算激活值的量化参数（主要是scale，即缩放因子）。

**2. 工作原理：**
1.  **准备阶段**：将模型的权重（恒定不变）提前转换为int8。
2.  **运行时阶段**：
    *   当输入数据传入模型时，系统会**实时观察**该次输入对应的激活值范围（min, max）。
    *   根据观察到的范围，**动态计算**出此次推理所需的量化参数（scale, zero_point）。
    *   使用这些刚计算出的参数，将激活值量化为int8。
    *   进行int8的矩阵乘加运算。
    *   将输出结果反量化为float32以供后续层使用或作为最终输出。

**3. 优点：**
*   **精度保留较好**：由于每次推理都根据实际的激活值范围来量化，能更好地适应输入数据的变化，通常比静态量化精度损失更小，尤其适用于激活值范围变化大的模型（如LSTM、Transformer）。
*   **无需校准数据**：因为它是在运行时动态计算的，所以不需要准备代表性的数据集来校准。

**4. 缺点：**
*   **速度提升有限**：因为在推理过程中需要动态计算激活值的量化参数，这会带来额外的开销。所以虽然计算本身是int8的，但总体加速比可能不如静态量化。
*   **不适合所有算子**：主要受益于Linear/FC层和LSTM层，对CNN的加速效果相对不明显。

**5. 典型应用场景：**
*   NLP模型，如BERT、LSTM，因为这些模型的激活值动态范围很大。
*   当你没有合适的校准数据集时。

---

### 静态量化

**1. 核心思想：**
*   **权重和激活都提前量化为int8。**
*   “静态”指的是：在**模型部署之前**，通过一个“校准（Calibration）”步骤，**一次性确定所有激活值的量化参数**，之后这些参数就固定不变了。

**2. 工作原理：**
1.  **准备阶段**：
    *   将模型的权重转换为int8。
    *   准备一个**具有代表性的校准数据集**（通常来自训练集或验证集的几百个样本）。
    *   运行“校准”过程：将校准数据送入模型，**统计所有中间激活层输出值的分布**（记录min/max或使用更高级的KL散度方法），从而为每一层确定一个**固定的、最优的**量化参数（scale, zero_point）。
2.  **部署/运行时阶段**：
    *   模型已经完全转换为int8模型（权重和激活都是int8）。
    *   推理时，输入数据直接使用第一步确定好的固定参数进行量化，然后进行纯int8计算，直到最后可能需要将输出反量化为float32。

**3. 优点：**
*   **性能最佳**：由于在运行时没有任何计算量化参数的开销，模型完全以int8运行，因此能获得**最大的速度提升**。
*   **极致优化**：整个计算图可以被深度学习编译器（如TVM, TensorRT）针对纯int8进行深度优化和融合。

**4. 缺点：**
*   **需要校准数据**：必须准备一个有代表性的数据集，如果校准数据与真实数据分布差异大，会导致严重的精度下降。
*   **灵活性差**：量化参数是固定的，如果遇到激活值分布与校准时有较大差异的输入，可能会产生较大的量化误差。

**5. 典型应用场景：**
*   计算机视觉模型，如CNN（ResNet, MobileNet等），因为这些模型的激活值分布相对稳定。
*   对推理速度有极致要求的场景。

---

### 对比总结表

| 特性 | 动态量化 | 静态量化 |
| :--- | :--- | :--- |
| **核心区别** | **运行时**动态计算激活值的量化参数 | **部署前**静态确定所有量化参数 |
| **权重** | 提前量化为 int8 | 提前量化为 int8 |
| **激活** | 运行时动态量化为 int8 | 提前量化为 int8 |
| **需要校准数据** | **否** | **是** |
| **性能/速度** | 较快，但有运行时开销 | **非常快**，无运行时开销 |
| **精度** | 通常更高，尤其适合动态范围大的模型 | 依赖校准数据，校准得好精度高 |
| **适用模型** | LSTM, BERT等NLP模型 | CNN（ResNet, MobileNet）等CV模型 |
| **部署复杂度** | 较低 | 较高，需要校准步骤 |

### 如何选择？

*   **如果你的模型是CNN，并且你有代表性的校准数据，追求极致性能** -> **选择静态量化**。
*   **如果你的模型是LSTM或BERT，或者你没有合适的校准数据，或者激活值范围变化很大** -> **选择动态量化**。
*   **如果你不确定** -> 可以两者都尝试，通过评估在测试集上的精度损失来做出最终决定。

在实际的框架中（如PyTorch、TensorFlow），它们都提供了相应的API来支持这两种量化方式。


问得非常好！**量化感知训练**是比静态/动态量化更高级、更有效的方法，它的核心思想是：**在模型训练阶段就模拟量化过程，让模型“提前知道”自己将来会被量化，从而学习到对量化误差更鲁棒的权重。**

### 核心思想：先苦后甜

让我们继续用厨师的比喻：

> - **静态/动态量化**：一位厨师（模型）已经用精密的天平（float32）训练成了顶级大厨。现在你突然给他换了一把粗糙的秤（int8），他做的菜味道可能就变了。
> - **量化感知训练**：在厨师**训练期间**，你就时不时地让他用那把粗糙的秤（模拟量化），让他从一开始就习惯在这种不精确的条件下做菜。最终，他训练出的厨艺（学到的权重）本身就兼容了粗糙秤的误差，等到真正换秤时，做的菜味道基本不变。

### 工作原理：模拟量化，欺骗梯度

QAT不是在训练时真的使用int8计算，而是在前向传播中**插入“假”的量化节点**来模拟精度损失，而在反向传播时仍然使用高精度来计算梯度。

**详细步骤：**

1.  **准备预训练模型**：从一个已经训练好的float32模型开始。

2.  **插入伪量化节点**：
    - 在模型图中，在需要量化的操作（如卷积、全连接层）的输入、输出和权重处，插入“伪量化”模块。
    - **前向传播**时，这些模块会执行以下操作：
        a. **模拟量化**：将float32的权重和激活值，通过之前介绍的量化公式（`q = round(r / scale + zero_point)`）转换为整数。
        b. **模拟反量化**：立即将这个整数再转换回float32。
        - 这个过程可以表示为：`r' = (round(r / scale + zero_point) - zero_point) * scale`
        - 注意：`r'` 是一个float32值，但它包含了量化引入的舍入误差。

3.  **微调训练**：
    - 使用训练数据对插入了伪量化节点的模型进行**微调**。
    - **前向传播**：模型感受到的是带有模拟量化噪声的数据，就像戴着镣铐跳舞。
    - **反向传播**：这里有一个关键技巧——**直通估计器**。
        - `round()`函数的梯度几乎处处为零，这会导致梯度无法传播。
        - STE将`round()`函数在反向传播中的梯度近似为1，即 `∂(round(x))/∂x ≈ 1`。
        - 这样，梯度就可以顺利通过伪量化节点，指导模型权重进行调整，以补偿量化带来的误差。

4.  **部署**：
    - 微调完成后，模型已经学会了在量化噪声下保持性能。
    - 此时，可以直接将模型转换为真正的、纯整数计算的int8模型进行部署，精度损失非常小。

### 量化感知训练 vs. 训练后量化

| 特性 | **训练后量化（PTQ）** | **量化感知训练（QAT）** |
| :--- | :--- | :--- |
| **时机** | 模型**训练完成后**进行 | 在模型**训练/微调阶段**进行 |
| **过程** | 静态/动态校准 | 插入伪量化节点进行微调 |
| **数据需求** | 静态量化需要少量校准数据；动态量化不需要 | 需要训练数据（或部分）进行微调 |
| **计算成本** | **低**，几分钟到几小时 | **中高**，需要额外的训练时间（几小时到几天） |
| **精度** | 通常有少量损失，对复杂模型损失可能较大 | **精度损失非常小，甚至无损** |
| **适用场景** | 快速部署，资源有限，对精度要求不极致 | 对精度要求极高，PTQ后精度不达标 |

### 为什么QAT效果更好？

因为PTQ是一种“事后补救”措施。模型权重是为高精度世界优化的，突然被量化后，它的决策边界可能会被破坏。而QAT是一种“事前预防”，模型在训练期间就主动将量化误差作为优化目标的一部分，从而找到一组在低精度世界里依然表现优异的权重。

### 总结与工作流程建议

在实际应用中，一个标准的模型优化流程通常是：

1.  **第一步**：在浮点数（fp32）下训练你的模型，直到收敛。
2.  **第二步**：尝试**训练后量化（PTQ）**，包括动态和静态量化。
3.  **第三步**：评估PTQ后模型的精度。
    - **如果精度达标** -> 完美！直接部署，成本最低。
    - **如果精度损失不可接受**（尤其是在模型较小或结构复杂时）-> **进入第四步**。
4.  **第四步**：进行**量化感知训练（QAT）**。
    - 在预训练的fp32模型上插入伪量化节点。
    - 使用训练数据进行微调（通常学习率要设得更低）。
    - 将QAT后的模型转换为真正的int8模型。
5.  **第五步**：部署最终的高精度、高性能的int8模型。

**总而言之，量化感知训练通过“将问题在训练阶段解决”的思路，牺牲了一些训练时间和成本，换取了部署时近乎无损的精度和极高的性能，是追求极致部署效果的首选方案。**

